%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = main.tex

% ===========================================================================================
\chapter{\textbf{Evaluation}}
\label{sec6:Evaluation}
In this chapter, the results from the sensitivity analysis and attacks seeded in ~\autoref{ch:Attacks} are presented. Also, CORGIDS is compared to its closely related work in terms of the metrics in ~\autoref{sec:metrics}.

\section{Sensitivity Analysis}
\label{sensitivityAnalysis}

Before evaluating CORGIDS, a sensitivity analysis to find out the values of the three experimental parameters was performed.  The result was chosen based on the highest values of precision and recall (\autoref{sec:metrics}). The three experimental parameters for which the sensitivity analysis was carried out are as follows.
\begin{itemize}
\item Window size (\textit{w}): A window size is defined as the time duration which is under consideration for detecting an intrusion~\cite{zohrevand2016hidden} in a SUT. A large \textit{w} means that greater historical data is given to the HMM to decide of a malicious activity.
\item Acceptable range ($\delta$): An acceptable range defines a range within which the testing system trace's likelihood can vary from the mean log likelihood from the trained HMM. A testing trace with the value within range from the specified mean will be marked to be similar to the training traces. If the value of $\delta$ is chosen to be large, it means that there is enforcement of loose control and allowing system traces with substantial variation from the trained HMM to be considered benign.
\item Threshold of consecutive decisions ($\lambda$): Stateful tests~\cite{urbina2016limiting} are performed by maintaining the historical decisions of the IDS and generating alert only if it goes above a chosen threshold. The intuition behind using the $\lambda$ is to look back at the historical decisions of the intrusion detector to see if there is really an anomaly or if it is just one time spike in the system. Greater value of $\lambda$ enforces more number of consecutive historical intrusion decisions to generate an alert.
\end{itemize}

The results from sensitivity analysis are shown in ~\autoref{fig:sensitivityAnalysis}. \textit{w} is measured in minutes while $\delta$ in standard deviations. A key point to note here is that more the value of precision and recall for a set of experimental parameters, higher is the rate of detection. Sensitivity analysis experiment was carried out by varying one parameter at a time and keeping others constant. For instance, graph \textit{a} in ~\autoref{fig:sensitivityAnalysis} denotes the scenario where the \textit{w} is varied from 2 to 4 minutes while keeping $\delta$ = 1 and $\lambda$ = 2. Similar to graph \textit{a}, the constant parameters will be swept , that is, $\delta$ and $\lambda$ from their lowest to the highest values. This forms the first row of graphs \textit{a} - \textit{d} in ~\autoref{fig:sensitivityAnalysis}. Similar to first row, other experiments were conducted by varying $\delta$ in second row and $\lambda$ in last row. This sensitivity analysis represents the data collected from the distance spoofing attack on the UAV testbed. Though similar analysis was performed for the other attacks on the UAV and SAP, due to space limitations they are not included here. 

From the graphs, it can be seen that the precision and recall are increasing as the \textit{w} is increasing from 2 to 4 minutes, while they are decreasing when the $\delta$ and $\lambda$ are increasing from 1 to 3 standard deviations and 2 to 4 decisions respectively. From this trend it can be inferred that the precision and recall are largest when the \textit{w} is large with small $\lambda$ and $\delta$. Similar trend was observed for other attacks on the two test-beds. The reason for the trend that was observed is that a HMM requires substantial historical data to determine if there is some anomaly in the system. With lesser history (smaller window size), it is unable to correctly infer the current state of the system. Therefore, when a greater \textit{w} of 4 minutes is provided, it is able to create a more realistic model of the system. As the HMM is now more confident about the system after having a large \textit{w}, it can now make decisions confidently, thus giving the best results for the least value of $\delta$ and $\lambda$.


An important point to note here is that though CORGIDS is able to detect attacks even with less favorable values of \textit{w}, $\lambda$ and $\delta$, it achieves less precision and recall in doing so. On the other hand, if the results obtained from sensitivity analysis are used, higher values of precision and recall can be achieved. Therefore, either the lesser favorable parameters can be chosen and results can be obtained  quickly at the cost of accuracy, or with the most favorable parameters, fewer FNs can be obtained, while incurring some latency.


\begin{figure}%
    \centering
    \subfloat[$\delta$ = 1 and $\lambda$ = 2]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Window_1.png} }}%
    \subfloat[$\delta$ = 3 and $\lambda$ = 2]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Window_2.png} }}%
    \subfloat[$\delta$ = 1 and $\lambda$ = 4]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Window_3.png} }}%
    \subfloat[$\delta$ = 3 and $\lambda$ = 4]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Window_4.png} }}%
    \qquad
    \subfloat[\textit{w} = 2 and $\lambda$ = 2]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Range_1.png} }}%
    \subfloat[\textit{w} = 4 and $\lambda$ = 2]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Range_2.png} }}%
    \subfloat[\textit{w} = 2 and $\lambda$ = 4]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Range_3.png} }}%
    \subfloat[\textit{w} = 4 and $\lambda$ = 4]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Range_4.png} }}%
    \qquad
    \subfloat[\textit{w} = 2 and $\delta$ = 1]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Decision_1.png} }}%
    \subfloat[\textit{w} = 2 and $\delta$ = 3]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Decision_2.png} }}%
    \subfloat[\textit{w} = 4 and $\delta$ = 1]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Decision_3.png} }}%
    \subfloat[\textit{w} = 4 and $\delta$ = 3]{{\includegraphics[width=0.26\textwidth]{Graphics/Spoofing_Decision_4.png} }}%
    \caption{Sensitivity Analysis: Variables are \textit{w}, $\delta$ and $\lambda$. The vertical axes in all figures are the values of precision and recall calculated after averaging 5 fold cross validation of test system traces.}%
    \label{fig:sensitivityAnalysis}%
\end{figure}


\section{Evaluation Criteria}
\label{sec:metrics}

Precision, recall, performance overheads and memory overheads were used to evaluate CORGIDS. These metrics are explained below:

\begin{itemize}
\item Precision: For a malicious execution of SUT, when an intrusion detector correctly detects an intrusion, is called precision. For an intrusion detector, the higher precision the better.
\item Recall: On the other hand, recall is the percentage when the SUT execution was malicious and the intrusion detector correctly identified it among all the malicious SUT executions. For an intrusion detector, the higher recall the better.
\item Performance Overhead: Performance overhead reflects the additional time taken, when CORGIDS is deployed on the SUT. It helps to determine if the time taken by the IDS to detect intrusion is greater than the time taken to complete a closed loop once, in which case it is not very helpful to use an IDS.
\item Memory Overhead: As the devices in which CORGIDS will be used will be memory constrained, it is essential to calculate its memory overhead. Memory occupied by CORGIDS on SUT will be used to determine this overhead.

As there wasn't a real UAV and I used a simulator for the UAV experiments, I did not calculate memory and performance overheads for UAV testbed.
\end{itemize}


\begin{table}
\centering
  \caption{False Positives and False Negatives obtained for CORGIDS on the two test-beds}
  \label{tab:results}
  \scalebox{0.9}{
  \begin{tabular}{|c|c|c|l|}
    \toprule
    \textbf{Testbed}&\textbf{Targeted Attack}&\textbf{FP (\%)}&\textbf{FN(\%)}\\
    \midrule
    \multirow{3}{*}{UAV}& Battery Tampering&0.0&12.20\\
                        & Flooding&0.0&11.30\\
                        & Distance Spoofing&0.0&12.80\\
    \hline
    \multirow{2}{*}{SAP}& Insulin Tampering&5.60&4.20\\
                            & Glucose Spoofing&2.80&8.40\\
    \hline
\end{tabular}
}
\end{table}


\begin{table}
\centering
  \caption{Comparison of Precision and Recall for OpenAPS platform}
  \label{tab:comparisonOfResults}
  \scalebox{0.9}{
  \begin{tabular}{|p{2.2cm}|p{2.0cm}|p{1.0cm}|p{1.0cm}|p{2.0cm}|p{1.8cm}|}
    \toprule
    \textbf{Methodology}&\textbf{Testbed}&\textbf{FP(\%)}&\textbf{FN(\%)}&\textbf{Precision(\%)}&\textbf{Recall(\%)}\\
    \midrule
    \multirow{2}{*}{ARTINALI}& SEGMeter&12&2.3&89.06&97.7\\
                        & OpenAPS&13.5&2&87.89&98\\

    \hline
    Zohrevand&Water&&& &\\
    et al.~\cite{zohrevand2016hidden}&Treatment&- &- & 78.87&81.4\\
     & System& & & & \\
    \hline
    Chen &Water &&&&\\
    et al.~\cite{chen2018learning}&Purification &- &15 &- &- \\
     &Plant& & & & \\
    \hline
    \multirow{2}{*}{CORGIDS}& UAV&0.00&12.10&100&87.90\\
                            & SAP&4.20&6.30&95.70&93.70\\

    \hline
\end{tabular}
}
\end{table}

For evaluating CORGIDS, the value obtained for each of the three variables (\textit{w}, $\lambda$ and $\delta$) from the sensitivity analysis was used. ~\autoref{tab:results} contains the results for False Positives (FP) and False Negatives (FN) for the two test-beds, namely, an UAV and SAP on which CORGIDS was deployed. ~\autoref{tab:comparisonOfResults} compares our results to only those related papers~\cite{chen2018learning,zohrevand2016hidden,aliabadi2017artinali} which dynamically generate physical invariants \footnote{Note: For the research papers for which the comparison is made against CORGIDS for the UAV and SAP testbed, I directly use the FPs, FNs, precision and recall evaluation metrics provided by them. I manually calculated precision and recall for~\cite{aliabadi2017artinali} from the FP and FN values provided in their paper}. I acknowledge that ~\autoref{tab:comparisonOfResults} does not provide an apples-to-apples comparison, but we include it here to provide better context about CORGIDS' performance.
Krotofil et. al. and Iturbe et. al.~\cite{krotofil2015process,iturbe2017feasibility} do not measure the performance of their methodology, and hence they could not be compared with CORGIDS. In addition, precision and recall for CORGIDS and the papers mentioned in ~\autoref{tab:comparisonOfResults} were calculated. To calculate the FP and FN values for CORGIDS which will be used to generate precision and recall, FP and FN values from ~\autoref{tab:results} were averaged. However, comparison of the precision value of CORGIDS with Chen et. al.~\cite{chen2018learning} could not be made, as the later did not provide it in their paper.  

\section{Experiment results} 
Based on the above described metrics, we now discuss the results of the experiments performed.

\subsection{RQ1. Precision}
In this subsection, the precision achieved by seeding the attacks on the SUT and using CORGIDS to detect an intrusion is discussed. Also,  the precision achieved with prior work in  ~\autoref{tab:comparisonOfResults} is shown. As can be observed, CORGIDS achieves a precision of 100\% and 95.70\% for the UAV and OpenAPS platform respectively. In comparison, no other intrusion detector has a precision greater than 90\%. Specifically, CORGIDS provides an 21.33\% improvement in precision over Zohravend et al. \cite{zohrevand2016hidden} and approximately 8.88\% over Aliabadi et al. \cite{aliabadi2017artinali} for the OpenAPS platform.

The reason behind the higher precision percentage for CORGIDS is the use of correlations exhibited by the two CPS. CORGIDS detects attacks by using an HMM to infer if the current system trace exhibits the same trend with which it was trained. This is the reason that especially for the UAV platform, the HMM recognizes an anomaly with almost 100\% precision. The reason for comparatively low precision value for OpenAPS platform is the lack of training traces. For the experiments on bot the test-beds, 70\%:30\% ratio for training and testing traces was maintained. However, the lack of availability of patient's diabetic therapy data led to a lower number of training traces. This, in turn negatively affected the training of the HMM used by CORGIDS for the OpenAPS platform.

\subsection{RQ2. Recall}
The recall factor of CORGIDS is discussed here and compared to the related work mentioned in ~\autoref{tab:results} and ~\autoref{tab:comparisonOfResults} respectively. From  ~\autoref{tab:comparisonOfResults}, it can be observed that CORGIDS receives a high recall percentage among all the related work. Although, CORGIDS does not have the highest recall, it is quite close to ARTINALI with 93.70\% for the OpenAPS platform. CORGIDS improves the recall by 15.11\% when compared to  Zohravend et al.~\cite{zohrevand2016hidden}. On the other hand, CORGIDS achieves 11.14\% lower recall than ARTINALI when both of them are compared with their lowest recall factors. 

 
CORGIDS achieves lesser recall than ARTINALI~\cite{aliabadi2017artinali} mainly because the behavior of the SUT under attack was stealthy and did not deviate much from the normal trend. As the deviation was less, the logical correlation between the properties seemed very similar to the one expected, thus the HMM did not mark the state as anomalous. As Chen et al. ~\cite{chen2018learning} do not provide recall factor, but give the value of FN for their approach, the FN value is compared to CORGIDS, which is 15\%. This is higher than CORGID's FN values of 12.10\% and 6.30\% for the two platforms. Chen et al. use SVMs for intrusion detection, while CORGIDS uses HMMs. HMMs are able to better capture the sequence of states and their transitions in a CPS, and hence CORGIDS achieves lower FN values.

\subsection{RQ3. Memory overhead}
Measurements of the memory overhead of CORGIDS running on the OpenAPS platform were also collected. The experiments were performed on a Raspberry Pi 3 with approximately 1 GB of RAM. We found that CORGIDS consumes 36.15 MB when detecting intrusion. The reason behind this memory overhead is that CORGIDS uses HMM for intrusion detection. The trained HMM model when loaded into memory along with the libraries required for it to generate a decision, requires more space. However, as CORGIDS is used by controller to detect intrusion in the SUT, and the controllers are not memory constrained as compared to the SUT. For instance, in Raspberry Pi 3, it took only a fraction (36.15 MB) of memory from the 1 GB available RAM. Thus, we surmise that the memory overhead incurred by CORGIDS is acceptable.

\subsection{RQ4. Performance overhead}
Like memory overhead, performance overhead measurements were also taken from the Raspberry Pi 3 platform. The average of 10 executions was considered for the overhead tests.  
Ideally, the time taken to deduce a decision should be less that the execution cycle of the SUT, in order for the intrusion detector to keep up with the system. It takes approximately 1.25 seconds for CORGIDS to generate a decision based on the input correlated logs. This is negligible compared to the time taken by a single execution cycle of OpenAPS, which is about 5 minutes.

\bigskip
\textbf{Scalability of CORGIDS:} To understand the scalability of the overheads with HMM size, the HMMs used for intrusion detection were varied. Thus, multiple HMMs were created by varying the tuning parameter, the number of \textit{hiddenStates}. The number of \textit{hiddenStates} were varied among {2, 5, 10, 15, 20}. It was observed that the memory and performance overhead of CORGIDS remains the same regardless of the number of \textit{hiddenStates} in the HMM. This is could be because the libraries which are loaded along with the HMM is the dominant factor in the time, and this does not depend on the number of \textit{hiddenStates} in the HMM.

\section{Additional experiments}
We carried out additional experiments to gather more insight about how the precision and recall vary based on the type of trained HMM.
We conduct experiments which include the variation of the two parameters which determine the type of trained HMM that will be generated.
\begin{itemize}
\item Training threshold - Training threshold is used as a stopping criteria for training the HMM in \textit{Building an Intrusion Detector} phase. This value signifies the maximum difference between current and previous HMM's log likelihood, if the current HMM is the trained HMM. We used the value of 0.5\% as the \textit{training threshold} in ~\autoref{sec3:Approach} based on the prior work~\cite{ferrer2000influence}. However, through this experiment we want to determine the effect that the change of this value has on intrusion detection capability of the trained HMM. We sweep the value of training threshold from the values (0.35\%, 1\% and 2\%) with 0.5\% value already being used in our experiments.

\item Number of training traces - The number of training traces determine the context that the trained HMM will have. Higher the number of training traces, more likely is that the HMM will be equipped with different types of behavior exhibited by the CPS. For our experiments, we kept the value of training to testing traces to be 70\% : 30\%. However, in this study we vary this ratio to see what effect it has on the precision and recall of the IDS. So we vary the ratio of training traces to testing traces between (50\% : 50\%, 60\% : 40\% and 80\% : 20\%) with 70\% : 30\% ratio already covered in the above experiments.
\end{itemize}

The two parameters discussed above are varied one by one, i.e. while one is varied other is kept constant, and vice-versa. Also, this study was conducted for both the platforms - UAV and SAP.

\subsection{Additional results for UAV platform}
We conducted two experiments, one for each of the two parameters and summarize the results below.

\begin{itemize}

\item \textit{Training threshold}: By varying the training threshold and keeping the number of training to testing traces constant to 70\% : 30\%, we got the result shown in ~\autoref{fig:UAV_threshold}. Similar trend of precision and recall was observed for each of the individual value of training threshold as when the training threshold was 0.5\%. Also, it was seen that the training threshold did effect the recall factor of CORGIDS. It can be seen that the recall increases as the training threshold decrease. So, for 0.35\% as the training threshold we get the maximum value of recall. However, the recall offered by training threshold value of 0.5\% is very close to the maximum recall value achieved in this experiments. Therefore, while using 0.5\% for the attacks which we performed in ~\autoref{ch:Attacks}, we did not sacrifice the intrusion detection capabilities of CORGIDS by using an under-trained HMM. On the other hand, it can also be seen that the precision remains unaffected by the change in training threshold, this could be because the trained HMM which we got from each of the individual experiments was well trained about the benign behavior of the SUT.

Another observation was made that the time taken to get the trained HMM model increased as the training threshold decreased. It was because, the maximum difference between the log likelihood's of the two HMM's was shrinking which led to creation of more HMM's until the threshold was met. However, as this training process needs to be done only once and that too on a non-constrained device, thus making the time consumed aspect not a road blocker.

\begin{comment}
\begin{table}
\centering
  \caption{Result by varying the \textit{training threshold} for the UAV platform}
  \label{tab:UAV_threshold_results}
  \scalebox{0.9}{
  \begin{tabular}{|c|c|c|}
    \toprule
    \textbf{Training threshold}&\textbf{Precision}&\textbf{Recall}\\
    \hline
    0.35\%& 100.0&90.70\\
    \hline
    0.5\%& 100.0 &89.20\\
    \hline
    1.0\%& 100.0&78.0\\
    \hline
    2.0\%& 100.0 & 74.50\\
    \hline
\end{tabular}
}
\end{table}
\end{comment}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.65,keepaspectratio = true]{Graphics/UAV_threshold.png}
    \caption{Result by varying the \textit{training threshold} for the UAV platform}
    \label{fig:UAV_threshold}
\end{figure}

\item \textit{Number of training traces}: By varying the training to testing traces ratio and keeping the training threshold at 0.5\%, we got the results described in ~\autoref{fig:UAV_traces}.
As can be observed, the number of training traces do affect the performance of CORGIDS. Specifically, CORGIDS achieve the best precision and recall when the training traces are maximum or near to the maximum value. Using 80\% or 70\% of the traces as training helped to achieve the recall of approximately 89 as opposed by lesser number of traces.

We see a dip in recall when the number of training traces decrease, because with fewer training data points, the HMM does not get all the possibilities of the behavior that it can expect of the SUT. Thus, more the number of training traces better the detection capability of CORGIDS. Additionally, training the HMM with different number of training traces did not make up a lot of time difference as compared to the experiment when the \textit{training threshold} was varied.

\begin{comment}
\begin{table}
\centering
  \caption{Result by varying the \textit{number of training traces} for the UAV platform}
  \label{tab:UAV_traces_results}
  \scalebox{0.9}{
  \begin{tabular}{|c|c|c|}
    \toprule
    \textbf{Number of training traces}&\textbf{Precision}&\textbf{Recall}\\
    \hline
    50\%: 50\% & 100.0 & 72.50\\
    \hline
    60\%: 40\% & 100.0 & 79.0\\
    \hline
    70\%: 30\% & 100.0 & 89.20\\
    \hline
    80\%: 20\% & 100.0 & 88.0\\
    \hline
\end{tabular}
}
\end{table}
\end{comment}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.65,keepaspectratio = true]{Graphics/UAV_traces.png}
    \caption{Result by varying the \textit{number of training traces} for the UAV platform}
    \label{fig:UAV_traces}
\end{figure}

\end{itemize}

\subsection{Additional results for SAP platform}
We performed similar experiments for the training threshold and the number of training traces variables for the SAP platform, with the results summarized below.

\begin{itemize}
\item \textit{Training threshold}: For this experiment, we used the same values of the training threshold from the above experiment involving the UAV platform. That is, we varied the threshold between (0.35\%, 1.0\% and 2\%) with the experiments already conducted for the 0.5\% value in the attacks discussed in the ~\autoref{ch:Attacks}. The results are summarized in ~\autoref{fig:SAP_threshold}.

We see that the variation in the training threshold value has an effect on the recall of CORGIDS. Particularly higher the value of training threshold, lower the recall of CORGIDS, which means it is not able to detect malicious activities that well. However, the precision for all the experiments led to approximately similar value of 95. This could be because the HMM was able to capture the benign behavior of the SUT with more precision. Further more, as the number of training traces for the SAP platform were approximately 4 times less than the UAV platform, it led to a very small number of training traces for each of the experiment conducted. Which is why the results of SAP platform if compared with UAV are lagging behind.

\begin{comment}
\begin{table}
\centering
  \caption{Result by varying the \textit{training threshold} for the SAP platform}
  \label{tab:SAP_threshold_results}
  \scalebox{0.9}{
  \begin{tabular}{|c|c|c|}
    \toprule
    \textbf{Training threshold}&\textbf{Precision}&\textbf{Recall}\\
    \hline
    0.35\% & 95.40 & 89.70\\
    \hline
    0.5\% & 95.70 & 93.70\\
    \hline
    1.0\% & 94.75 & 75.0\\
    \hline
    2.0\% & 95.30 & 57.50\\
    \hline
\end{tabular}
}
\end{table}
\end{comment}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.65,keepaspectratio = true]{Graphics/SAP_threshold.png}
    \caption{Result by varying the \textit{training threshold} for the SAP platform}
    \label{fig:SAP_threshold}
\end{figure}

\item \textit{Number of training traces}: By varying the ratio of training to testing traces and keeping the training threshold fixed at 0.5\%, we conducted experiments whose results are shown in~\autoref{fig:SAP_traces}. It is clearly seen that the number of training traces surely impacted the precision and recall of CORGIDS. As anticipated, the few training traces led to less contextual behavior absorption for the HMM which ultimately reflected on the evaluation metrics. For the attacks and the result shared in ~\autoref{ch:Attacks}, we used 70\% : 30\% as the ratio and as can be seen from ~\autoref{tab:SAP_traces_results}, the precision and recall attained are at the top.

\begin{comment}
\begin{table}
\centering
  \caption{Result by varying the \textit{number of training traces} for the SAP platform}
  \label{tab:SAP_traces_results}
  \scalebox{0.9}{
  \begin{tabular}{|c|c|c|}
    \toprule
    \textbf{Number of training traces}&\textbf{Precision}&\textbf{Recall}\\
    \hline
    50\%: 50\% & 67.50 & 62.50\\
    \hline
    60\%: 40\% & 72.35 & 72.60\\
    \hline
    70\%: 30\% & 95.70 & 93.70\\
    \hline
    80\%: 20\% & 96.0 & 92.60\\
    \hline
\end{tabular}
}
\end{table}
\end{comment}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.65,keepaspectratio = true]{Graphics/SAP_traces.png}
    \caption{Result by varying the \textit{number of training traces} for the SAP platform}
    \label{fig:SAP_traces}
\end{figure}

\end{itemize}

\endinput
=====================================================================
% EOF